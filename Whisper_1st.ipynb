{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNCaTkWwY5uO09l2J8hfDx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/OpenAI/blob/main/Whisper_1st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper - 1st practice (22.12.20)\n",
        "\n",
        "[Video tutorial](https://www.youtube.com/watch?v=wrSelk44_Js)\n",
        "\n",
        "[Whisper license](https://github.com/openai/whisper/blob/main/LICENSE) - Copyright (c) 2022 OpenAI\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"),..."
      ],
      "metadata": {
        "id": "ta_H1AqoCdQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video tutorial (Play if you want)"
      ],
      "metadata": {
        "id": "vKkMerYWCuiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxnPzg74CXlT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"wrSelk44_Js\", width=500)\n",
        "display(video)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "github.com/openai/whisper"
      ],
      "metadata": {
        "id": "7ZvUPmAsDJN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/openai/whisper.git "
      ],
      "metadata": {
        "id": "1xku6V5oDKA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Base model access"
      ],
      "metadata": {
        "id": "laXNqxU-ECmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload speech file from your computer\n",
        "\n",
        "[sample speech short - rainbow passage](https://github.com/MK316/OpenAI/blob/main/audiodata/sample02.wav)"
      ],
      "metadata": {
        "id": "TrIKxVDqEntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "B-o4ZHmKDlFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model('base.en')\n",
        "result = model.transcribe(\"sample02.wav\", language=\"en\",fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "5bKpmuHGErrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Transcribing Korean audio"
      ],
      "metadata": {
        "id": "9mwQ97osGD1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Korean speech file using gTTS"
      ],
      "metadata": {
        "id": "_Qh8P1j1GDR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "cArdnfU4GOHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸš© Making a function { tts ( _text_to_say_) }:\n",
        "def tts(text):\n",
        "\n",
        "  !pip install gTTS\n",
        "  from gtts import gTTS\n",
        "  from IPython.display import Audio\n",
        "\n",
        "  text_to_say = text\n",
        "\n",
        "# Step â“µ Language to choose:\n",
        "  language_to_choose = \"ko\" #@param [\"en\", \"fr\",\"ko\",'es']\n",
        "  # lang = language_to_choose\n",
        "  print(\"Play language accent: %s\"%language_to_choose)\n",
        "  language = language_to_choose\n",
        "\n",
        "# gTTS\n",
        "  gtts_object = gTTS(text = text_to_say,\n",
        "                     lang = language,\n",
        "                    slow = False)\n",
        "  \n",
        "# #@markdown Step â‘¢: Create the audio file (.wav) to play:\n",
        "  gtts_object.save(\"sample.wav\")\n",
        "\n",
        "# # Output\n",
        "  return Audio(\"sample.wav\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mlMeZGApGlc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* text1: \"ë°¤ì´ ëŠ¦ì—ˆìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ì¬ë¯¸ìˆëŠ” ì½”ë”©ì„ ë°°ìš°ëŠ” ì¤‘ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¼ì„œ ì˜ êµ¬í˜„ë ì§€ ëª¨ë¥´ê² ë„¤ìš”.\"\n",
        "\n",
        "* text2: \"ì°©í•˜ì§€ë§Œ ê°€ë‚œí•œ ë™ìƒê³¼ ìš•ì‹¬ ë§ì€ í˜•ì˜ ì´ì•¼ê¸°ì¸ í¥ë¶€ë†€ë¶€ì „. ë‚´ìš©ì„ ê°„ë‹¨íˆ ì†Œê°œí•˜ìë©´ í¥ë¶€ëŠ” ì˜¨ê°– ì–´ë ¤ì›€ ì†ì—ì„œë„ ì°©í•œ ë§ˆìŒì„ ìƒì§€ ì•Šì•˜ì§€ë§Œ, ë†€ë¶€ëŠ” ëê¹Œì§€ ìš•ì‹¬ì„ ë¶€ë¦¬ë‹¤ê°€ ë²Œì„ ë°›ê²Œ ëœë‹¤. ì°©í•œ í¥ë¶€ëŠ” ë²Œì„ ë°›ì€ í˜•ì„ ë²„ë¦¬ì§€ ì•Šê³  ë„ì™€ì£¼ê³ , ë’·ë‚  í¥ë¶€ëŠ” ì°©í•œ ì‚¬ëŒì„ ëŒ€í‘œí•˜ëŠ” ì´ë¦„ì´, ë†€ë¶€ëŠ” ìš•ì‹¬ ë§ì€ ì‚¬ëŒì„ ëŒ€í‘œí•˜ëŠ” ì´ë¦„ì´ ë˜ì—ˆë‹¤. ë‚˜ìœ ë§ˆìŒì”¨ë¥¼ ê°–ê³  ì‚´ë©´ ë²Œì„ ë°›ê³ , ì°©í•˜ê²Œ ì‚´ë©´ ë³µì„ ë°›ëŠ”ë‹¤ëŠ” ì „í˜•ì ì¸ ì „ë˜ë™í™”ì˜ êµí›ˆì„ ë‹´ê³  ìˆë‹¤.\""
      ],
      "metadata": {
        "id": "nVAwVHP7NtAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸš© Type text to say\n",
        "print('Type texts to create audio:')\n",
        "txt = input()\n",
        "tts(txt)"
      ],
      "metadata": {
        "id": "vxobpRf-G2MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Recording was done manually on Praat by playing the generated gTTS audio (ko).\n",
        "\n",
        "[sample audio](https://github.com/MK316/OpenAI/blob/main/audiodata/sample_ko_01.wav)\n",
        "\n",
        "[text2 audio](https://github.com/MK316/OpenAI/blob/main/audiodata/sample_k_shortstory_ttsvoice.wav)  \n",
        "\n",
        "* _Note_: the sentence was copied to see whether the error comes from the boundary of audio files."
      ],
      "metadata": {
        "id": "Jwfy6lUIHu5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('base')\n",
        "result = model.transcribe('sample_ko_01.wav', language=\"ko\", fp16=False)\n",
        "print(result['text'])"
      ],
      "metadata": {
        "id": "YhU6d5TNIHI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe('sample_ko_01.wav', language=\"ko\",task = 'translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "bH4Hi7I2PXGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample 2: short story\n",
        "\n",
        "* Text: \"ì°©í•˜ì§€ë§Œ ê°€ë‚œí•œ ë™ìƒê³¼ ìš•ì‹¬ ë§ì€ í˜•ì˜ ì´ì•¼ê¸°ì¸ í¥ë¶€ë†€ë¶€ì „. ë‚´ìš©ì„ ê°„ë‹¨íˆ ì†Œê°œí•˜ìë©´ í¥ë¶€ëŠ” ì˜¨ê°– ì–´ë ¤ì›€ ì†ì—ì„œë„ ì°©í•œ ë§ˆìŒì„ ìƒì§€ ì•Šì•˜ì§€ë§Œ, ë†€ë¶€ëŠ” ëê¹Œì§€ ìš•ì‹¬ì„ ë¶€ë¦¬ë‹¤ê°€ ë²Œì„ ë°›ê²Œ ëœë‹¤. ì°©í•œ í¥ë¶€ëŠ” ë²Œì„ ë°›ì€ í˜•ì„ ë²„ë¦¬ì§€ ì•Šê³  ë„ì™€ì£¼ê³ , ë’·ë‚  í¥ë¶€ëŠ” ì°©í•œ ì‚¬ëŒì„ ëŒ€í‘œí•˜ëŠ” ì´ë¦„ì´, ë†€ë¶€ëŠ” ìš•ì‹¬ ë§ì€ ì‚¬ëŒì„ ëŒ€í‘œí•˜ëŠ” ì´ë¦„ì´ ë˜ì—ˆë‹¤. ë‚˜ìœ ë§ˆìŒì”¨ë¥¼ ê°–ê³  ì‚´ë©´ ë²Œì„ ë°›ê³ , ì°©í•˜ê²Œ ì‚´ë©´ ë³µì„ ë°›ëŠ”ë‹¤ëŠ” ì „í˜•ì ì¸ ì „ë˜ë™í™”ì˜ êµí›ˆì„ ë‹´ê³  ìˆë‹¤.\"  \n",
        "[online source](https://m.post.naver.com/viewer/postView.naver?volumeNo=9154443&memberNo=15460571)  \n",
        "[audiofile info](https://raw.githubusercontent.com/MK316/OpenAI/main/audiodata/audiofile_info.md)"
      ],
      "metadata": {
        "id": "O44t25LXKf3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text2 audio (by human, female)"
      ],
      "metadata": {
        "id": "qQOAawR0OvL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('base')\n",
        "result = model.transcribe('sample_k_shortstory.wav', language=\"ko\", fp16=False)\n",
        "print(result['text'])"
      ],
      "metadata": {
        "id": "txEF7oomKtI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "translate task"
      ],
      "metadata": {
        "id": "5Eu1-kGOP3RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe('sample_k_shortstory.wav', language=\"ko\",task = 'translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "73u10ICCP2jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text2 audio (by gTTS, male)"
      ],
      "metadata": {
        "id": "sqZz_S-nOyuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('base')\n",
        "result = model.transcribe('sample_k_shortstory_ttsvoice.wav', language=\"ko\", fp16=False)\n",
        "print(result['text'])"
      ],
      "metadata": {
        "id": "EbnhARY4OuDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Low level model access"
      ],
      "metadata": {
        "id": "SJz-163CNLfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('small')\n",
        "\n",
        "audio = whisper.load_audio('sample_ko_01.wav')\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-mel spectrogram and move to the same device as the model.\n",
        "\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n"
      ],
      "metadata": {
        "id": "cB8I4NHOMWyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Language detection:\n",
        "\n",
        "[Trained data](https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg)"
      ],
      "metadata": {
        "id": "lKKtHzh2Qpje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect the spoken language\n",
        "\n",
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key = probs.get)\n",
        "prob = \"{0:.0%}\".format(max(probs.values()))\n",
        "\n",
        "# print language that scored teh hightest liklihood\n",
        "\n",
        "print(f'Detected language (and probability): {lang}', f'({prob})')"
      ],
      "metadata": {
        "id": "cruy9sMqQooB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] Korean to English translation (Text1 is excellent but the result of Text2 is very poor.)"
      ],
      "metadata": {
        "id": "PXCwFqiuRo2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the audio\n",
        "\n",
        "options = whisper.DecodingOptions(language=\"ko\", task = 'translate')\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "VlfF4KPzRqwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}